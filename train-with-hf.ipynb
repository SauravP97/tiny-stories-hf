{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd3620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the TinyStories dataset\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "\n",
    "# Check the structure\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f52c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'Running on: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82418f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use GPT-Neo tokenizer (standard practice for this replication)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Truncate to a small context length (e.g., 512) as stories are short\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize a subset for quick testing\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952f53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312038af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GPTNeoConfig\n",
    "\n",
    "# Define a \"Tiny\" configuration\n",
    "config = GPTNeoConfig(\n",
    "    vocab_size=len(tokenizer),        # Match GPT-2 vocab (~50k)\n",
    "    max_position_embeddings=512,      # Context window (stories are short)\n",
    "    hidden_size=64,                   # Very small embedding dimension\n",
    "    num_layers=8,                     # Only 4 transformer blocks\n",
    "    num_heads=16,                     # 4 attention heads\n",
    "    attention_types=[[['local'], 8]]\n",
    ")\n",
    "\n",
    "# Initialize the model from scratch (NOT pre-trained)\n",
    "model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "print(f\"Model parameters: {model.num_parameters() / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b02af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "# Data Collator handles dynamic padding\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tiny-stories-model\",\n",
    "    num_train_epochs=1,              # 1 epoch is often enough for this dataset to see convergence\n",
    "    per_device_train_batch_size=8,  # Reduced batch size to mitigate OutOfMemoryError\n",
    "    save_steps=5000,\n",
    "    logging_steps=500,\n",
    "    learning_rate=5e-4,              # Slightly higher LR for small models\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    fp16=True,                       # Use Mixed Precision if on GPU\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02f1a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Start training the model from scratch\n",
    "# trainer.train()\n",
    "\n",
    "# To resume from a checkpoint\n",
    "checkpoint_path = \"/content/tiny-stories-model/checkpoint-55000\"\n",
    "trainer.train(resume_from_checkpoint=checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Prompt with a typical TinyStories opening\n",
    "prompt = 'A cat performs a new trick for her friends but starts shivering.'\n",
    "\n",
    "# prompt = \"Once upon a time, there was a little girl named Lily.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=200, \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
