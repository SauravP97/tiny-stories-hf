{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edfc4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use GPT-2 tokenizer (standard practice for this replication)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e04c13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96933b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 108/108 [00:00<00:00, 1608.67it/s, Materializing param=transformer.wte.weight]                       \n"
     ]
    }
   ],
   "source": [
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\"./checkpoint-55000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "694772ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 64)\n",
      "    (wpe): Embedding(512, 64)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-7): 8 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=64, out_features=50257, bias=False)\n",
      ")\n",
      "Model parameters: 3.65M\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model)\n",
    "print(f\"Model parameters: {model.num_parameters() / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61faf1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saurav and Muskan went on a date. There was a big house set of toys, games and books. Kim and beans were very happy.\n",
      "\n",
      "One day, they couldn't find their favorite toy. They wanted to buy something new. They looked around and saw the toy he had a toy. The toy was very expensive. Kim and Fred wanted the toy.\n",
      "\n",
      "\"Let's share it with both,\" said Freddy. They both smiled and hugged the toy.\n",
      "\n",
      "A few days later, when they were done playing, the toy disappeared. Kim and dad were so sad. They looked up and saw the toy was gone.\n",
      "\n",
      "\"Let's go to the toy house,\" Kim said.\n",
      "\n",
      "\"I'm sorry,\" their dad said.\n",
      "\n",
      "\"Let's go back to the living room,\" dad said.\n",
      "\n",
      "Sally and their dad left the toy and went to their house. They saw a big box filled with some toys inside. They were\n"
     ]
    }
   ],
   "source": [
    "# Move model to evaluation mode\n",
    "pretrained_model.eval()\n",
    "\n",
    "# Prompt with a typical TinyStories opening\n",
    "prompt = \"Saurav and Muskan went on a date\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(pretrained_model.device)\n",
    "\n",
    "# Generate\n",
    "outputs = pretrained_model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=200, \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
