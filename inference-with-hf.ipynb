{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edfc4096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravprateek/Documents/saurav-codes/neural-nets-codelab/saurav-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Use GPT-2 tokenizer (standard practice for this replication)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e04c13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96933b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 108/108 [00:00<00:00, 2841.42it/s, Materializing param=transformer.wte.weight]                       \n"
     ]
    }
   ],
   "source": [
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\"./checkpoint-55000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694772ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 64)\n",
      "    (wpe): Embedding(512, 64)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-7): 8 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (v_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (q_proj): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (c_proj): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=64, out_features=50257, bias=False)\n",
      ")\n",
      "Model parameters: 3.65M\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_model)\n",
    "print(f\"Model parameters: {pretrained_model.num_parameters() / 1_000_000:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61faf1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saurav and Muskan went on a date with their mom. She was wearing a big bag, and she liked to play with them. She had a lot of toys, but they also wanted more.\n",
      "\n",
      "She asked her mom if she could play with her and mom said yes. Sara was excited and she said yes. She took her mom's bag and ran to the park. She saw a small bush and asked her mom if they could see it.\n",
      "\n",
      "\"Sure, honey!\" Sara said. She pointed at the bush and saw a big tree in the tree. She opened it and saw a tree with a big tree. She thought the tree was very pretty.\n",
      "\n",
      "She decided to come and play with her toys. She put her toys in the tree and went to the tree. She looked around and saw a bird with a bird. She smiled and said, \"Look, mom! The bird was so pretty!\"\n",
      "\n",
      "Her mom smiled and said, \"Yes\n"
     ]
    }
   ],
   "source": [
    "# Move model to evaluation mode\n",
    "pretrained_model.eval()\n",
    "\n",
    "# Prompt with a typical TinyStories opening\n",
    "prompt = \"Saurav and Muskan went on a date\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(pretrained_model.device)\n",
    "\n",
    "# Generate\n",
    "outputs = pretrained_model.generate(\n",
    "    inputs.input_ids, \n",
    "    max_length=200, \n",
    "    do_sample=True, \n",
    "    temperature=0.7, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543827d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00, 72.53it/s]\n",
      "Processing Files (1 / 1): 100%|██████████| 14.6MB / 14.6MB, 49.2kB/s  \n",
      "New Data Upload: 100%|██████████| 14.6MB / 14.6MB, 49.2kB/s  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SauravP97/tiny-stories-3M/commit/76ba007b25ac3c92a92c3409c48d4825fafb4478', commit_message='Upload GPTNeoForCausalLM', commit_description='', oid='76ba007b25ac3c92a92c3409c48d4825fafb4478', pr_url=None, repo_url=RepoUrl('https://huggingface.co/SauravP97/tiny-stories-3M', endpoint='https://huggingface.co', repo_type='model', repo_id='SauravP97/tiny-stories-3M'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_name = 'tiny-stories-3M'\n",
    "pretrained_model.push_to_hub(repo_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saurav-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
